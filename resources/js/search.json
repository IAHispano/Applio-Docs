[[{"l":"Home","p":["At the forefront of innovation as an open-source ecosystem that hosts cutting-edge AI voice cloning technologies.","Welcome to Applio - the ecosystem of voice cloning tools meticulously optimized for unrivaled power, modularity, and a user-friendly experience."]}],[{"i":"beginnerss-guide","l":"Beginners's Guide","p":["This section is oriented to users new to the world of AI voice conversion."]}],[{"l":"Frequent Doubts"},{"i":"what-is-applio-and-where-can-i-use-it","l":"What is Applio and where can I use it?","p":["Applio is the application that will allow us to create our own voices with AI or use existing voices. The program can be used installing it on your pc, or from the cloud through Google Colab (the best alternative for users who do not have a computer or have low resources).","As mentioned at the end of the text, to use applio locally on your computer you must have good hardware, especially if you want to clone a voice, what software do you need?. For Tranining applio requires at least a Nvidia Series 2000 (RTX) graphics card.","Keep in mind that if you want to use voices in your computer you can do it without having a super hardware."]},{"i":"how-do-i-install-applio","l":"How do I install Applio?","p":["Downloading applio is a very simple process, you can watch this video tutorial or view the installation section.","There is also a Google Colab alternative, if you do not have much idea how colab works or how to perform some actions within the environment you can have a look at the section of other alternatives."]}],[{"l":"Interface","p":["One of the main features of applio for user comfort is a simple to understand interface, but how can I make...?, this is what you will find here.","Make an inference/use voices; you can go to the inference section.","Make a Dataset, train a Voice Model: training Guide/ Create a Dataset","Use custom Pretrained (like Ov2 and RIN_E2): Load custom Pretrained","Use text-to-speech conversion for your Models: TTS Guide","How to use and understand the Tensorboard: Tensorboard Guide","Use the Audio Analyzer Tool for your datasets: Audio Analyzer Guide","Combine models to create a new one: Voice Blender Guide"]}],[{"l":"Make your first model","p":["Here you will learn the basics to create your first model.","Learn the basics of RVC/Applio: All about RVC/ How to use Applio.","Create your dataset: Guide on creating a dataset.","Learn how to easily extract vocals or audio Audio Isolating Guides.","Verify the sample rate of your dataset: Learn how to check it.","Train your model while monitoring TensorBoard: training Guide/ Tensorboard Guide.","Finally, test it: Inference Guide."]}],[{"l":"Make AI Cover","p":["Here you will learn how to make AI covers.","Separate the vocals from the instrumental: Audio Isolating Guides","Search or download the model you want to use: Applio Bot/ Applio Web","Open Applio and make the cover: Inference Guide/ Other Alternatives, then combine it with the instrumental and you’re done."]}],[{"l":"Get Started with Applio"}],[{"l":"Installation","p":["Click on run-applio.bat, that’s all.","Desactivate your antivirus and firewall to avoid missing dependencies.","Don't open the Applio.exe or run-applio.bat as an administrator.","Don't put it in a folder with privileged access.","Don't run the run-applio.bat as an administrator.","Don't run the run-install.bat as an administrator.","Download here","Download the .bat file of the latest version from GitHub repository","Download the zip here","Extract it","Extract the zip file","Here you will learn how to install Applio using commands in CMD.","Here you will learn how to install Applio via the official repository.","If you are having problems trying to run the executable, try with the Exe format.","If you are having problems trying to run the executable, try with the Zip format.","If you encounter an error during execution, you can try the precompiled (Exe) version of Applio.","If you encounter an error during execution, you can try the precompiled (Zip or Exe) version of Applio.","If you encounter an error during execution, you can try the precompiled (Zip) version of Applio.","In case you get a Windows message, just click on More information and then on Run anyway.","Installing Applio is a simple process, you can download and install Applio in different ways. If you are new to Applio we recommend installing the precompiled version (Zip or Exe) as it comes ready to use Applio.","Make sure that you place Applio inside a folder on C driver.","Make sure the path does not contain any spaces or special characters.","Open the executable so it can extract the program files","Run the run-applio.bat file","Run the run-applio.sh file","Run the run-install.bat, wait for it to download the necessary content","Run the run-install.sh, wait for it to download the necessary content"]},{"i":"how-to-update-applio","l":"How to update Applio?","p":["To update, it is necessary to delete the current Applio folder and reinstall it. Make sure to save your audios and models before deleting the current folder."]}],[{"i":"infer--download-models","l":"Infer & Download models","p":["Downloading a voice model is as simple as going to the Download tab.","Manual loading alternative: unzip the downloaded .zip file and drag the two files into the Drop files box. Now you can search for models from Applio by simply entering the character name to search in the Search Model section, then copy the model link or download it.","Applio can only download models from Google Drive, Hugging Face, Applio Web, Discord, Yandex, Pixeldrain and Mediafire. Downloading files from mega may fail!, this is due to malfunction of the dependency."]},{"l":"How to infer","p":["When we have the model ready we will go back to the inference section.","Upload your audio and select it, make sure it is only vocals.","Click the Refresh button at the top right and select the downloaded/uploaded files in the Voice model and index file box.","Press Convert and you will have your inference done!","Place your audio files in a folder, copy and paste the path into the Input Folder, if you’d like, you can also specify the output path for the converted audios in the Output Folder.","Recommendation: having a clean acapella helps to get better results. but if the model downloaded in its name mentions the words rvmpe or crepe click on the advanced options box. Furthermore, this section has the following:","Look for better quality audio.","Your voice model needs more training or is overtraining.","Remove the reverb, double vocals and noise from your acapella, check the UVR 5 guide or MVSEP guide.","The dataset of your model contained noise, you need to clean the dataset.","Try advanced settings."]},{"i":"advanced-settings","l":"**Advanced Settings**","p":["Export Format: Select the format to export the audio.","Split Audio: Basically cuts the audio into parts to make the inference by parts and then joins them together.","Autotune: Apply a soft autotune to your inferences, recommended for singing conversions.","Clean Audio: Clean your audio output using noise detection algorithms, recommended for speaking audios.","Upscale Audio: Upscale the audio to a higher quality, recommended for low-quality audios.","Clean Strenght: The more you increase it the more it will clean up, but it will be more compressed.","Pitch: Adjust the tone of the model, for male it is - and female it is +. For male to female is -12 and female to male is +12.","Filter Radius: Applies respiration filtering to the results, the value represents the filter radius and respiration reduction to avoid artifacts.","Search Feature Ratio: It is the one in charge of controlling the index, the larger the ratio, the more single the dataset but it can result in artifacts, so it is better to leave it as it is by default.","Volume Envelope: Substitute or blend with the volume envelope of the output.","Protec Voiceless Consonants: Safeguard distinct consonants and breathing sounds to prevent electro-acoustic tearing and other artifacts.","Hop Length: Denotes the duration it takes for the system to transition to a significant pitch change. Smaller hop lengths require more time for inference and training but tend to yield higher pitch accuracy.","Pitch extraction algorithm: Select between rvmpe, crepe or other","(optional) Embedder Model: select the Embedder model (contentvec, japanese-hubert-base, chinese-hubert-large or custom)."]}],[{"l":"Training","p":["Training is only for NVDIA GPUs, If you don’t have a compatible GPU, the Training tab will be disabled.","If you don’t have an NVIDIA GPU, you can try Applio Colab or Applio No UI Colab. Check the other alternatives section.","Upload your audio in .wav format using the Dataset Maker (if it is a single audio) or setup it manually (various audios) to applio/assets/datasets creating inside a folder for the program to read it.","don't know how to create a dataset?, check the dataset section","Once the model is named and the dataset selected press \"Prepocess Dataset\" and wait for the message in the CMD.","you can select one of the 3 available frequencies according to the audio (32k, 40k, 48k), this will help to avoid filtering out more artifacts or background noise.","don't know how to check sample rate?, check the sample rate section","Select an F0 method that suits your needs.","(optional) modify Hop lenght, lower value, higher smoothness of pitch change but slower training and vice versa.","(optional) select the Embedder model (contentvec, japanese-hubert-base, chinese-hubert-large or custom)","Configure the training parameters according to your needs.","Save Every Epoch: Set this value between 10 and 50 to determine how often the model's state is saved during training.","Total Epochs: The number of epochs needed varies based on your dataset. Monitor progress using TensorBoard; typically, models perform well around 200-400 epochs.","Batch Size: Adjust based on your GPU's VRAM. For 8 GB VRAM, use a batch size between 6 and 8. Consider CUDA cores when experimenting with higher batch sizes."]},{"l":"Other Options","p":["Custom Pretrained: Uses the Custom Pretrained that are loaded.","don't know how to use Tensorboard for correct training? Check out the Tensorboard section","GPU Settings: Allows to choose GPUs (only for users who have more than one GPU).","Now you can export your trained model directly from the Applio interface, go to the Export Model section in the train tab, click on the Refresh button and select the pth and the added index of the model to export.","Once configured, press 'Start training' to start the process, everything is registered in the CMD.","Once training is completed, generate the index file by clicking the \"Train Feature Index\" button.","Open Applio if you have closed it.","Overtraining Detector: Mark it only if you will train for more than 200 epochs.","Overtraining Threshold: Set the maximum number of epochs you want your model to stop training if no improvement is detected.","Pitch Guidance: Gives variation of pitch.","Pretrained: Uses the RVC pretrained.","Save Every Weights: Save the weights of the model when a cycle of 'Save Every Epoch' is completed.","Save Only Latest: Save a single D/G file with information.","Sync Graph: Synchronize the graph of the tensorbaord. Only enable this setting if you are training a new model.","Then, in the Applio interface, input your model name, use the same sample rate, and proceed to the last part of the train tab. Set the same batch size, pretrained (if you used) and increase the number of epochs you want to train.","Your trained model is located in the logs/model folder, and the .pth files are in the logs/zips folder."]}],[{"l":"TTS","p":["We need to go to the TTS section in Applio.","Select our model with its respective index.","Select the model of TTS depending on the language you want and enter the text or add a txt file in the box below to convert.","Select the TTS Speed(increase or decrease TTS speed).","Finally we click on convert and wait for the result","Applio also features ElevenLabs TTS. Check the Plugins section for more information."]},{"i":"advanced-settings","l":"**Advanced Settings**","p":["Export Format: Select the format to export the audio.","Split Audio: Basically cuts the audio into parts to make the inference by parts and then joins them together.","Autotune: Apply a soft autotune to your inferences, recommended for singing conversions.","Clean Audio: Clean your audio output using noise detection algorithms, recommended for speaking audios.","Upscale Audio: Upscale the audio to a higher quality, recommended for low-quality audios.","Clean Strenght: The more you increase it the more it will clean up, but it will be more compressed.","Pitch: Adjust the tone of the model, for male it is - and female it is +. For male to female is -12 and female to male is +12.","Filter Radius: Applies respiration filtering to the results, the value represents the filter radius and respiration reduction to avoid artifacts.","Search Feature Ratio: It is the one in charge of controlling the index, the larger the ratio, the more single the dataset but it can result in artifacts, so it is better to leave it as it is by default.","Volume Envelope: Substitute or blend with the volume envelope of the output.","Protec Voiceless Consonants: Safeguard distinct consonants and breathing sounds to prevent electro-acoustic tearing and other artifacts.","Hop Length: Denotes the duration it takes for the system to transition to a significant pitch change. Smaller hop lengths require more time for inference and training but tend to yield higher pitch accuracy.","Pitch extraction algorithm: Select between rvmpe, crepe or other","(optional) Embedder Model: select the Embedder model (contentvec, japanese-hubert-base, chinese-hubert-large or custom)."]}],[{"l":"Pretrained"},{"i":"what-are-pretraineds","l":"What are pretraineds?","p":["When it comes to training, you have two main options: building a model from the ground up or fine-tuning an already existing one. These pre-trained models are designed to streamline your training process, saving time and enhancing the overall quality of your results.","Simply put, the image illustrates that having a pre-trained model saves you effort during subsequent model training."]},{"i":"how-to-create-pretraineds","l":"How to create pretraineds?","p":["When creating a pretrained model, you have two primary options to consider.","Firstly, you can either initiate a fine-tuning process on another pretrained model, which could be one of the originals, or start building one from scratch.","Should you opt for building from scratch, the ideal approach involves gathering a substantial amount of moderately clean data; it doesn't necessarily have to be perfectly pristine. Subsequently, fine-tune this model with high-quality data.","An essential consideration is to construct datasets devoid of copyrighted material.","Alternatively, if you choose to fine-tune a pretrained model, the crux lies in the quality of the audio inputs. You can tailor it to a specific language, incorporate diverse speakers, and even integrate various accents. The customization possibilities are vast. However, it's crucial to strike a balance; avoid overtraining the pretrained model. The more effectively you fine-tune it now, the less training it will necessitate later during usage.","To embark on building a model from scratch, conduct standard training while disabling the Pretrained option. For fine-tuning, engage in ordinary training while loading the desired pretrained model to fine-tune it."]},{"i":"how-to-use-pretraineds","l":"How to use pretraineds?","p":["In the training tab, check the 'Custom Pretrained' box, upload the files, and select it in the Pretrained G/D Path boxes.","Now you can download the custom pretrained models directly from the Download tab in Applio, just select the Pretrained model you want to download, choose the Sample Rate, and click on Download."]},{"i":"where-to-find-pretraineds","l":"Where to find pretraineds?"},{"l":"Ov2 Super by SimplCup","p":["Works well for small and English datasets. Additionally, it allows models to train with very few epochs compared to regular pretrains. This only supports 32k and 40k sample rates.","https://huggingface.co/ORVC/Ov2Super/tree/main"]},{"l":"RIN E3 by Mustar","p":["Yields good results with clean and English datasets, but it is more sensitive to noise. This only supports 40k sample rate.","https://huggingface.co/MUSTAR/RIN_E3/tree/main"]},{"l":"SnowieV3 by Mustar","p":["Works well with Russian datasets and also helps models of other languages to pronounce Russian well. Supports all the sample rates.","32k: https://huggingface.co/MUSTAR/SnowieV3.1-32k/tree/main","40k: https://huggingface.co/MUSTAR/SnowieV3.1-40k/tree/main","48k: https://huggingface.co/MUSTAR/SnowieV3.1-48k/tree/main"]},{"l":"SnowieV3 X RIN_E3 by Mustar","p":["Works well with Russian or Japanese language datasets and also helps models of other languages to pronounce Russian or Japanese well. This only supports 40k sample rate.","https://huggingface.co/MUSTAR/SnowieV3.1-X-RinE3-40K/tree/main"]},{"l":"KLM v7 by SeoulStreamingStation","p":["Works well with Korean and some types of Japanese voices. Unlike typical pre-trained models, it has a very wide vocal range, which should be very helpful for those covering songs, it supports all the sample rates.","https://huggingface.co/SeoulStreamingStation/KLM4/tree/main"]},{"l":"TITAN by blaise-tk","p":["It gives cleaner results compared to the original pretrained, also handles the accent better. Like Ov2 Super, it allows models to be trained with few epochs, it only supports all the saple rates.","32K: https://huggingface.co/blaise-tk/TITAN/tree/main/models/medium/32k/pretrained","40K: https://huggingface.co/blaise-tk/TITAN/tree/main/models/medium/40k/pretrained","48K: https://huggingface.co/blaise-tk/TITAN/tree/main/models/medium/48k/pretrained","Make sure to select the sample rate according to the sample rate of the custom pretraineds."]}],[{"l":"Tensorboard","p":["Tensorboard is a series of graphs where we can monitor the progress of our model during training, but there are many graphs. We are only interested in the graph called 'g/total'. You can find this by clicking on 'inactive' and selecting 'scalars'. Then, go to the last page, where you will find it in the last graph."]},{"i":"how-tu-use-it","l":"How tu use it?","p":["Open TensorBoard by running the following command in your terminal or use the run-tensorboard.bat file:","Replace \"path/to/your/logs\" with the actual path to your TensorBoard logs.","Once TensorBoard is running, open your web browser and navigate to http://localhost:6006(or the address indicated in your terminal).","Click on the \"Scalars\" tab in TensorBoard.","Look for the \"g/total\" metric at the top to monitor the progress of your training.","TensorBoard Screenshot","This image provides a visual guide for locating the \"g/total\" metric within the \"Scalars\" tab.","Set the smoothing to 0.950 or 0.987 for a better view of the graph.","You can click on the ⚙️ to mark the option to reload data every 30 seconds.","Below each graph, there will be 3 buttons. The first one is to put it in full size, the second one is to deactivate the Y-axis and the last one is to adjust the data to the graph.","Uncheck the option to ignore outliers in chart scaling."]},{"l":"Lowest Point","p":["It’s when the graph goes to a point so low that it doesn’t happen again. During training, there will be several low points which you should test to find the indicated pth of your model and in this way prevent our model from being overtrained. To know which one to choose, we go to the lowest point and look at how many steps it has. Knowing this, we can search in the open cmd or colab notebook for the epochs with that step or the closest one from the save points."]},{"l":"Advanced Information","p":["all/mel: A visualization of the Mel-spectrogram of the entire target audio.","Besides checking the loss/g/total, it’s necessary to monitor the loss/g/mel, loss/g/kl and loss/d/total graphs. If any of these values increase and don’t decrease again, it indicates overtraining. Following this, we will explain more details about these and some parts of TensorBoard:","General Trend: Look for a decreasing trend in the generator's total loss ( loss/g/total) and a relatively stable or increasing trend in the discriminator's total loss ( loss/d/total) over time.","grad_norm_d: The gradient norm for the discriminator. A measure of the magnitude of gradients during training. It helps monitor if the gradients are becoming too large (potentially causing instability) or too small (potentially leading to slow learning).","grad_norm_g: The gradient norm for the generator. Similar to grad_norm_d, this measures the magnitude of gradients for the generator.","Gradient Norm: Monitor the gradient norms to ensure they don't become excessively large or small.","learning_rate: The current learning rate of the optimizers for both the generator and discriminator. This value will typically decrease over time as defined by the learning rate scheduler.","Loss Components: Analyze individual loss components (e.g., loss/g/mel, loss/g/kl) to understand how specific aspects of the model are performing.","loss/d_g/{i}: Individual discriminator losses for generated audio at each discriminator output. These losses indicate the discriminator's ability to distinguish generated audio from real audio at various temporal scales.","loss/d_r/{i}: Individual discriminator losses for real audio at each discriminator output. These losses show how well the discriminator is performing on real audio at different scales.","loss/d/total: The total discriminator loss. This loss reflects how well the discriminator can differentiate between real and generated audio. A higher value usually indicates a well-performing discriminator.","loss/g/{i}: Individual generator losses for each discriminator output. These losses provide a more detailed breakdown of the generator's performance at different temporal scales.","loss/g/fm: The feature matching loss. This loss encourages the generator to produce audio that matches the feature maps extracted by the discriminator, promoting similar acoustic characteristics between real and generated audio.","loss/g/kl: The Kullback-Leibler (KL) divergence loss. This loss measures the difference between the distributions of the latent variables generated by the encoder and the posterior encoder. A lower value indicates better alignment between these distributions, contributing to a more stable and controllable model.","loss/g/mel: The Mel-spectrogram loss. This loss measures the difference between the Mel-spectrogram of the generated audio and the target Mel-spectrogram. A smaller value indicates a better match in spectral characteristics.","loss/g/total: The total generator loss. This measures how well the generator is able to fool the discriminator and produce realistic-sounding audio. A lower value is generally better.","Mel-Spectrograms: Compare the Mel-spectrogram images ( slice/mel_org and slice/mel_gen) to visually assess the quality of the generated audio.","slice/mel_gen: A visualization of the Mel-spectrogram of the generated audio segment.","slice/mel_org: A visualization of the Mel-spectrogram of the target audio segment."]}],[{"l":"Audio Analizer","p":["To use this tool, we go to the Extra section in the Audio Analyzer, drag our dataset and click on Get audio information.","don't understand how it works?, check the sample rate section"]}],[{"l":"Voice Blender","p":["Go to the Voice Blender section, place the Model Name of your new model, drag or upload the pth A and B files below, adjust the Audio Blend if desired, and click the Fusion button.","Your new pth file can be found in Applio/logs, ready for use in inference. Please note that being a mixed model, it won’t have an index."]}],[{"l":"Plugins"},{"i":"what-are-plugins-in-applio","l":"What are plugins in Applio?","p":["Plugins or extensions, as the name implies, allow you to add functions to applio without having to update the program.","How do I upload extensions to Applio?","Uploading Extensions to Applio is a simple process, download the extensions you want from here, go to the Plugins section and drag the .zip file to Plugin Installer, When you upload the file and refresh the interface you will see the section of the extension you have downloaded.","(Manual Upload: Drag and drop the files to Applio/Tabs/Plugins)"]},{"l":"Update Plugins","p":["Drag the updated zip file to the upload box or upload the folder to Applio/Tabs/Plugins/installed"]}],[{"l":"Extra","p":["Here you will find more things you can do in Applio."]},{"l":"View Model Information","p":["Go to the Extra section in the Processing section and place the model path in Path to model and click on View. This will display the model data in the command prompt (cmd)."]},{"l":"Settings","p":["Go to the Settings section where you can modify the following aspects:","Enable Applio presence on Discord.","Activate the ability to download models directly from the Applio Web by clicking on","Set your preferred theme.","Check the installed version of Applio.","Change the language of the Applio interface to your preference."]}],[{"l":"Other Alternatives","p":["**Advanced Settings**","**Other Options**","Afterward, run the Generate index file cell.","auto_backups (recomended): Keep this checked it if you want to save a copy of your model in your Google Drive after training completes.","AutoBackup: Saves our trained model to our drive. Drive needs to be mounted first.","Autotune: Apply a soft autotune to your inferences, recommended for singing conversions.","Batch_size: Adjust the batch size according to your preference; a batch size of 8 is mostly recommended.","Clean_audio: Clean your audio output using noise detection algorithms, recommended for speaking audios.","Clean_strenght: The more you increase it the more it will clean up, but it will be more compressed.","Click on \uD83D\uDCC1 and locate your trained model in the Applio/logs/model folder, and the .pth files are in the Applio/logs/zips folder.","Click on \uD83D\uDCC1 in Colab, and create a new folder named pretrained_custom inside the pretraineds folder located in Applio/rvc/pretraineds.","Click on \uD83D\uDCC1 in Colab, then navigate to your audio, copy the path, it should look like this: /content/drive/MyDrive/audio name.wav and paste it into the input_path. Finally run the Run inference cell.","Custom_pretrained: Keep this checked if you want to use the Custom Pretrained that are loaded.","don't know how to use Tensorboard for correct training? Check out the Tensorboard section","Download Custom Pretrains: Allows direct downloading of custom pretrains to Applio.","Download the pretrained you want to use and upload the d.pth and g.pth files to your Google Drive.","Due to recent issues with Gradio links, a shared_tunnel has been implemented. If you select this option, you’ll need to access the shared link and enter the Password IP. If you leave it unchecked, a public link will appear to enter the interface, in case it doesn't appear, use the other method.","Enter Applio Colab and just run the 2 cells.","Enter Applio No UI Colab and run the Installation cell.","Entering the last 3 commands from step 4.","Export_format: Select the format to export the audio.","F0method: Select between rvmpe, crepe or other","f0up_key: Adjust the tone of the model, for male it is - and female it is +. For male to female is -12 and female to male is +12.","Filter_radius: Applies respiration filtering to the results, the value represents the filter radius and respiration reduction to avoid artifacts.","Finally in the Train set the same batch size, custom_pretrained (if you used), increase the number of total_epoch you want to train and other seetting that you used before and run that cell.","Finally your trained model will be located in your Google Drive within the RVC_Backup folder.","Finally, copy the path of G.pth and paste it into g_pretrained_path, and do the same for D.pth into d_pretrained_path. It should look like this: /content/Applio/rvc/pretraineds/pretraineds_custom/G48k.pth and /content/Applio/rvc/pretraineds/pretraineds_custom/D48k.pth, make sure to select the sample rate according to the sample rate of the custom pretraineds.","Finally, the link to access the GUI will appear.","Finished Model: go to the Export Model section in the Train tab, click on the Refresh button and select the pth and the added index of the model to export. Finally, click on the Upload button, your model will be found in the ApplioExported folder ready to be used.","First you need to create a hugging Face account.","First you should create an account on Paperspace","First, you’ll need to upload the audio you want to use to your Google Drive. Make sure the audio name don't contain spaces or special characters.","Go to the Gradient tab and click on the Create button:","Google Colab (Google Colaboratory) is a free cloud based platform that enables writing and executing Python code within an interactive notebook environment. It provides free access to computing resources, including CPUs, GPUs and TPUs from Google’s servers, making it particularly useful for machine learning tasks and data analysis. Additionally, Colab offers a Pro subscription, but it’s not highly recommended to pay for it when there are better alternatives available. Some Colabs have an interface (UI) through Gradio, providing a simple and user-friendly experience for users, however, these interfaces are not allowed by Google Colab, so there is a possibility of jeopardizing your google account.","Google Colab (Google Colaboratory) is a free cloud based platform that enables writing and executing Python code within an interactive notebook environment. Some Colabs do not use interfaces (No UI) because their use is banned in Google Colab, which means that the entire process is done directly from the notebook using cells, making it challenging for some users.","Hop_length: Denotes the duration it takes for the system to transition to a significant pitch change. Smaller hop lengths require more time for inference and training but tend to yield higher pitch accuracy.","Hugging Face is a company specialized in artificial intelligence that has distinguished itself through its work in developing tools and models for natural language processing (NLP). This has made it one of the leading online communities and platforms for cutting edge NLP model research, development and distribution.","If the model still needs training, simply do the following:","In the Train section, before run that cell you should set the following:","In this section, you will find the following options:","Index_rate: It is the one in charge of controlling the index, the larger the ratio, the more single the dataset but it can result in artifacts, so it is better to leave it as it is by default.","Load a Backup: Used to load our model if we want to retrain it. Drive must be mounted.","Locate your pretrained model on your Google Drive and drag it into the newly created folder.","Make sure to set the batch size according to the VRAM of your chosen GPU.","Manual (NOT RECOMENDED): click on \uD83D\uDCC1 in Colab and locate the added index of your model in the program/logs/model folder, and the .pth files are in the program/logs/zips folder so you can download them. If you want to save your model folder in your Drive, you just need to cancel the Start Applio cell and run the Mount Drive cell to be able to move it to your Google Drive.","Model to continue training: if you want to save all the archives of your model folder in your Drive, you just need to run the Mount Drive and AutoBackup cell before run Start Applio cell, your model folder will be found inside the ApplioBackup folder.","Mount Drive: Connects Colab to our Google Drive for performing backups.","Next time, you will only enter the last 3 commands.","Next, in the Extract Features section, set the f0method and hop_length, and run that cell.","Now type the following commands one at a time in the terminal; this is only for the first time:","Once the account is created, you will need to choose a Paperspace plan. To do this, click on Upgrade and select the one that suits your preference. The best option is the Pro plan.","overtraining_detector: Mark it only if you will train for more than 200 epochs.","overtraining_threshold: Set the maximum number of epochs you want your model to stop training if no improvement is detected.","Paperspace has joined DigitalOcean, so you can also pay with PayPal or Google Pay!.","Paperspace is a cloud platform that provides access to powerful virtual machines with dedicated GPU for software development, machine learning, artificial intelligence, and other applications that require substantial processing power. This has made it one of the most competent paid alternatives to Colab.","Pitch_guidance: Gives variation of pitch.","Place the Google Drive or Hugging Face model link in the model_link and run the Download model cell.","Place your audio in a folder on your Google Drive and click on \uD83D\uDCC1 in Colab, then navigate to your dataset folder, copy the path, it should look like this /content/drive/MyDrive/dataset name folder and paste it into the dataset_path, make sure the audio and folder name don't contain spaces or special characters. Also, specify the sample rate and run the Preprocess Dataset cell.","Place your audio in a folder on your Google Drive, run the Mount Drive cell in Colab and click on \uD83D\uDCC1 in Colab. Then navigate to your dataset folder, copy the path, it should look like this /content/drive/MyDrive/dataset folder and paste it into the dataset path.","Please note noted that Gradio is banned from Colab, so it is preferable to use a secondary, a new account or Applio No UI Colab to avoid a possible ban. Additionally, it is possible that Colab may disconnect after a few hours due to Google’s limitations.","Pretrained: Uses the RVC pretrained.","Protect: Safeguard distinct consonants and breathing sounds to prevent electro-acoustic tearing and other artifacts.","Rms_mix_rate: Substitute or blend with the volume envelope of the output.","Run tensorboard in another terminal, click on and enter the following commands:","Run the Installation cell as before, but now you’ll need to enter your model name in Load a backup cell and run it.","Run the same cells as before, but now you’ll need to enter your model name in Load a backup and run it.","Save Every Weights: Save the weights of the model when a cycle of 'Save Every Epoch' is completed.","Save Only Latest: Save a single D/G file with information.","Save_every_epoch: Set this value between 10 and 50 to determine how often the model's state is saved during training.","Select a avaible GPU","Select the number of hours your computer will remain active. Then click on Start Notebook.","Select the template: Start from Scratch","Split_audio: Basically cuts the audio into parts to make the inference by parts and then joins them together.","Sync Graph: Synchronize the graph of the tensorbaord. Only enable this setting if you are training a new model.","tensorboard: Keep this checked; it will determine if the model is overfitting.","Thanks to our team, we were able to bring Applio to other places for your convenience","Then enter Applio Hugging Face Space and duplicate the space, that is all. If you don't want to use hugging Face you can use Applo playground.","Then, in the Applio interface, input your model name, use the same sample rate, and proceed to the last part of the train tab. Set the same batch size, pretrained (if you used) and increase the number of epochs you want to train.","Then, in the Set training variables cell, input your model_name, use the same sample_rate and hop_length amd run it.","To load your dataset in Colab, there are two ways to do it:","To run this code in Colab, we have to press the keys Ctrl + Shift + i to open the developer tools option and the following window will open. In case another window is displayed, we can locate it by selecting the Console option, so in this way Colab will not disconnect during the separation process. First, we put Allow pasting, then we press enter and only then it will let us enter the code.","To save your model, there are three ways to do it:","Total_epoch: The number of epochs needed varies based on your dataset. Monitor progress using TensorBoard; typically, models perform well around 200-400 epochs.","Upload your audio in .wav format using the Dataset Maker or click on \uD83D\uDCC1 and setup it manually to Applio/assets/datasets creating inside a folder for the program to read it.","Upload your audio in .wav format using the Dataset Maker or click on \uD83D\uDCC1 in Colab and setup it manually to program/assets/datasets creating inside a folder for the program to read it.","You can pay by the hour for a dedicated GPU instead of using the default free ones or the paid plan.","You can train models while checking Tensorboard, do inference, or use TTS","You can't train models here because it only utilizes CPU.","You should know that Paperspace’s free plan is quite poor, and there’s nothing you can do about it. So, you should consider paying for a Pro or Growth plan.","You will be asked for account verification, so you will need to enter a phone number and input the received verification code.","You will need to clarify some additional account details before starting to use Paperspace."]}],[{"l":"Guides","p":["This section contains guides to help in the process of creating voices and other processes."]}],[{"l":"Audio Isolating","p":["To create a model, we first need to create our dataset. To do this, we need to separate the voices from the music. Here are some tools that can help you:","Download the music in .WAV format, or if it's in .mp3 format, convert it to .WAV."]},{"l":"Audio Isolating Tools"}],[{"l":"UVR","p":["5_Hp-Karaoke-UVR (to remove choruses from vocals)","Also you can download the music that you want to separate from Colab, go to the YouTube Audio Downloader for Separation cell, just place the link of the song in video_url and song name in audio_name, then run that cell. Your song will be stored in the Separar folder that you created in your Drive.","BS Reformer","BS-Reformer-Viperx-1297 (the best to remove instrumental and vocals)","Choose Process Method select the type of models you need to use (MDX-Net or VR Arch) after that select the model and click Start Processing.","Demucs","Download and install Ultimate Vocal Remover.","Download the video of the song you want to use as a basis for creating the dataset or the AI cover:","Finally, look for your processed audio in the location you selected as output.","First, we go to the UVR Colab.","For Youtube: copy the song link from the search bar and paste it onto Cobalt.","Here you will learn how to Separate the Vocals and the Instrumental of songs with UVR.","htdemucs (to remove instrumental, vocals, drums, bass and other)","htdemucs_ft (to remove instrumental, vocals, drums, bass and other)","If you wish, you can change the Overlap in case of BS Reformer, the Segment Size and the Overlap in case of MDX23C and MDX-NET, the windows size and the aggression setting for VR Arc, the Shifts and Overlap in case of Demucs to your preference or leave it as it is.","If you wish, you can change the Segment Size and the Overlap in case of MDX23C and MDX-NET, the windows size and the aggression setting for VR Arc to your preference or leave it as it is.","In case you get a Windows message, just click on More information and then on Run anyway.","In case you want to separate directly from YouTube, place the link in the audio input, otherwise leave it as it is.","In our Google Drive, we created the folder Separar where we will put our songs to separate, and Vocales where our already separated song will be stored.","MDX-NET models","MDX23C","MDX23C 8KFFT InstVoc HQ (one of the best to remove vocals and instrumental)","MDX23C 8KFFT instVoc HQ2 (one of the best to remove the instrumental and vocals)","MDX23C-InstVoc HQ (the best to remove the instrumental and vocals)","Mel-Reformer-Viperx-1143 (to remove instrumental and vocals)","Now we go to the Vocales folder, there we find our separate audios, we download the audio with the name of the song-Vocals, in case you want the instrumental is song-Instrumental.","Open Ultimate Vocal Remover and download the models, with which you can separate the voices and the instrumental parts into separate audios. For that, you will have to click here \uD83D\uDD27 and we go to the Download Center section.","Run the cell of your choice: BS Reformer, MDX23C, MDX-NET, VR ARCH, Demucs.","Run the Installation cell to start cloning the repository to the colab and mount drive.","Select the audio or song as input and choose the output folder for the instrumental or final vocals.","UVR-BVE-4B_SN-44100-1 (to separate 2 or more voices singing at the same time)","UVR-DeEcho-DeReverb (to remove reverb from vocals)","UVR-DeNoise (to remove noise from vocals)","UVR-MDX-NET Crowd HQ 1 (to remove crowd from vocals)","UVR-MDX-NET Voc FT (to remove the instrumental and vocals)","VR Arch models","You can change the type of wav or the MP3 bitrate in Additional settings, in Audio Format Settings.","You can choose the output format format of your separate audio.","You can select if you only want the Vocals Only, Instrumental Only, the vocals with No Reverb Only, No Crowd Only, No Noise Only or with those effects."]}],[{"l":"MVSEP","p":["After registering and logging in, select your audio or music to separate, and then choose the type of separation and the type of model if requested.","BanditPlus (to remove SFX, speech, music, effects)","BSReformer V2 (vocals, instrumental)","First, we go to the MVSEP page.","Here you will learn how to Separate the Vocals and the Instrumental of songs with MVSEP.","In some cases, you will see the option of aggressiveness, for better results you should set it to 0.5.","MDX B (vocals, instrumental)","MDX23C (to remove vocals and instrumental)","MVSep MelBand Reformer (to remove vocals and instrumental)","Then we click on the separate button and wait until it finishes to download our final vocals.","Ultimate Vocal Remover HQ (vocals, music)","Unlike doing it locally, here you will have to wait in a queue, this may take time depending on the queue at that moment.","UVR-BVE-4B_SN-44100-1 (to separate 2 or more voices singing at the same time)","UVR-DeEcho-DeReverb (to remove reverb from vocals)","UVR-DeNoise (to remove noise from vocals)","UVR-MDX-NET Voc FT (to remove instrumental and vocals)","ver 2024,04 (the best to remove instrumental and vocals)"]}],[{"l":"KaraFan","p":["/ KaraF/ KaraFan_user","/ KaraFan","/ KaraFan_user","/ KaraFan_user / Multi_Song","/ Music","/ Results","After installation, you'll find new folders at the root of the folder you have chosen with the following structure :","Choose « Properties »","Click on the « OK » button","Donwload:","First, we go to the Google Colab.","First, we have to download and install Python.","Folder","For more information, visit the GitHub repository.","Function","Here you can put your audio files (or choose another one)","Here you will find your extracted audio files (or another one)","Here you will learn how to Separate the Vocals and the Instrumental of songs with KaraFan.","If you have installed Python 3.11 before WITHOUT using this Install.bat, you must uninstall it before ! Even those installed with the Microsoft Store !","If you want to test SDR with your own recipes","In our Google Drive, we created the folder Music where we will put our songs to separate, and Vocals where our already separated song will be stored","In the « General » tab, click on the « Unlock » button","Install.bat<-- Click to show, save it and rename to Install.bat.","Install.exe<-- The easiest way! (auto-extract the BAT).","Intall.zip<-- Bypass the Browser Security","Its is only for NVDIA GPUs.","Now place the settings you want to apply, try the ones you want the most, but if you want to effectively separate the instrumental or sounds from the vocals, I recommend the following settings:","Now we go to the Results folder and we will find a folder with the name of our song or audio. There we will find a list of audios, we download the audio with the name Vocal Final, in case you want the instrumental one that says Music Final.","Now we go to the Vocals folder and we will find a folder with the name of our song or audio. There we will find a list of audios, we download the audio with the name Vocal Final, in case you want the instrumental one that says Music Final.","Now we have to copy the path of your song that you want to separate and the path of the folder where the separated song will be stored.","Now, you can launch KaraFan.pyw by double-clicking on it.","On Windows 11, you must allow first the execution of the BAT file:","On your PC where you wish to store the KaraFan project and Go inside it.","Place your audio in the Music folder and copy the path of your song that you want to separate and the path of the folder where the separated song will be stored.","Right-click on the file","Run Setup.py by double-clicking on it.","Run the cell by clicking on the white button. So that the repository can start to be cloned.","Simply Run the Setup.py file!","The Colab only lasts 30 minutes in execution. Additionally, it is possible that Colab may disconnect after a few hours due to Google’s limitations.","The first time you use it, it will start downloading the models you have selected in your drive in the KaraFan_user folder. If you keep this folder for the next time you want to use it, you will not need to download it again.","The main application","Then we click on the next cell This is it! to load the KaraFan interface.","To run this code in Colab, we have to press the keys Ctrl + Shift + i to open the developer tools option and the following window will open. In case another window is displayed, we can locate it by selecting the Console option, so in this way Colab will not disconnect during the separation process. First, we put Allow pasting, then we press enter and only then it will let us enter the code.","We click on Start and it will begin to separate the chosen song. At the end of the process, the message Processing DONE will appear at the end of the interface.","Where all the models are stored","Where config file & FFmpeg executable are saved","You must execute it with Administrator Rights."]}],[{"l":"Create datasets","p":["Here you will learn how to create a dataset to train your first RVC model."]}],[{"l":"How to create datasets"},{"i":"what-is-a-dataset","l":"What is a Dataset?","p":["A Dataset is a set of audio files compressed into a .zip file used by RVC for voice training.","First, we need to search for or download the audios of the person or character you want to create the model for","The audio must be in WAV or FLAC format and must be of good quality","The dataset should have a minimum duration of 10 minutes or a maximum of 30 minutes for better results","Make sure your dataset has the greatest variety of tones so that the model does not struggle when imitating certain tones.","Now we just need to separate the vocals from the instrumental.","don't know how to isolate audios?, check the audio insolation guide","After separating our audios, we open Audacity to edit them and do the following:","The dataset should only contain speech; any other type of sound should be removed or reduced, such as hisses, screams or breathing.","Finally, we export our already processed audio and it’s ready for training."]},{"l":"Noise Gate","p":["Is a tool that helps reduce background noise in an audio recording, first select your audio and go to the effect section under the Steve Daulton option and select noise gate. Then set the following parameters"]},{"l":"Truncating silence","p":["This tool is used to remove silences from our dataset, now go back to the effect section under the special option and select truncate silence. Then set the following parameters:"]}],[{"l":"Sample Rate","p":["Here you will learn how to see the sampling frequency of an audio."]},{"i":"what-is-sample-rate","l":"What is sample rate?","p":["Basically, it’s the frequency that our audio can reach, which is measured in KHz. If you choose an incorrect sample rate to train a type of sound unrelated to the voice, it can result in a poor model.","To do this, we need the program Spek. If you have Applio installed on your PC, you can check the audio analyzer section.","Drag your dataset into the program, and you’ll get the following image and information:","Sample rate","Duration of the audio","Number of Samples","Bits per Samples","Channels (Mono or Stereo)","Now that we’ve seen in the Spectrogram image that our audio has an audio frequency of 22050 KHz, we multiply it by two, and we would have our sample rate.","In case you get a frequency like 44100k, you should choose the one that is closest to that frequency."]}],[{"l":"Other Guides","p":["In addition to Applio, we have a section of guides related to RVC or artificial intelligence. More information here. There you will find guides like:","W-Okada Voice Changer","Stable Cascade","Illusion Difusion AI","DeepFaceLab","LM Studio","Stable Video Diffusion-XT","Stable Diffusion","Whisper"]}],[{"l":"Voice Models","p":["You can access voice models on various Discord servers dedicated to AI, such as AI Hispano.","Additionally, you can explore the capabilities of the official Applio bot for more information. Click here to learn more about this bot!"]}],[{"l":"Applio Bot","p":["The bot is capable of searching models within a database of over +500k functional RVC voice models, consider using Applio Bot. It offers the convenience of collecting models from multiple servers and provides an easy to use interface."]},{"l":"Invitation"},{"l":"How to use Applio Bot","p":["Type the command /search, enter the model name you want to search for, Click on \uD83D\uDCE4Download to download the model or right click to get the model link, that’s all."]}],[{"l":"Applio Web","p":["If you are not closely related to discord or some other reason you can choose to use the web version of the Applio, search for any RVC voice model in our database.","Choose the model you wish to use and click on to get the link or on to download it, then go to the Download tab of Applio."]}],[{"l":"Developers","p":["This section is oriented to users who are interested in our work."]}],[{"l":"Applio API","p":["Create, experiment and develop; fast and public."]},{"i":"what-is-the-applio-api","l":"What is the Applio API?","p":["Applio API is the key that allows request and access in a few clicks a quantity of +20k voice models created using RVC technology totally free. This is done through our database, with which the Applio Bot and the Website works.","How to access and use the Applio API?","Go to the Applio website, register, and once you have done so, you can go to the API section and create a key."]}],[{"l":"Projects","p":["\"I have programming knowledge and I would like to contribute to applio projects, is there any possibility?\"","Of course, all projects like Applio Bot, Applio, even the docs themselves are public and open source so that anyone can contribute improvements or features to them!, you can find them at https://github.com/IAHispano"]}],[{"l":"Report an Error","p":["\"I have an error when using Applio, is there any way to report it or find a solution?\"","Yes, you can report a bug by going to the issues section, click on \"new issues\", send an image / full text of the Applio CMD and comment how you get the error (in which part, what action perform when getting the message).","You can also go to the report a bug section in Applio to easily record what is happening. - Applio tool to record an error"]}],[{"l":"Frequently Asked Questions","p":["We've curated a comprehensive list of frequently asked questions or resolved errors to address any queries you might have. If you still have doubts or need further assistance, please don't hesitate to reach out to us via our official Discord server."]}],[{"l":"General","p":["Questions and errors about RVC/Applio in general"]}],[{"l":"Errors","p":["Common general errors in RVC/Applio."]},{"i":"dependencies","l":"**Dependencies**","p":["In the conda installation method, Applio is installed in a conda environment. This enviroment has all the dependencies needed to run RVC/Applio so you don't need to install anything else."]},{"i":"error-downloading-miniconda","l":"**Error Downloading Miniconda**","p":["If there are issues automatically downloading Miniconda, download it manually and install it."]},{"i":"conda-error-building-wheels","l":"**Conda Error Building Wheels**","p":["If Conda shows an error related to the \"Wheels\" dependency, check that Visual Studio Build Tools are installed. This requirement it should be installed semi automatically by the installer."]},{"i":"tensorboard-or-dependency-error","l":"**Tensorboard or dependency error**","p":["If an error related to Tensorboard occurs, it's likely that no dependencies were installed correctly. Review if you have the latest installer version.","Also, please check if you have done any of the following:","If you did, please reinstall applio in a directory without spaces or non ASCII characters in the path. (For example your C: drive root)","If you did, please reinstall applio with the antivirus disabled.","If you did, please open it without administrator privileges."]},{"i":"situation-with-google-colab","l":"**Situation with Google Colab**","p":["Unfortunately, Google Colab is currently unavailable as Google has taken measures to restrict access to Colab services, including Applio. Thanks to our team, we have been able to bring Applio back. More information here."]},{"i":"ffmpeg-error-utf8-error","l":"**FFmpeg error/utf8 error**","p":["Ensure paths are free of special characters."]},{"i":"connection-error","l":"**Connection Error**","p":["Ensure the console (black command line window) is open."]},{"i":"webui-popup-expecting-value-line-1-column-1-char-0","l":"**WebUI popup 'Expecting value: line 1 column 1 (char 0)**","p":["Disable system LAN proxy/global proxy and refresh."]},{"i":"cuda-error-cuda-out-of-memory","l":"**Cuda error/Cuda out of memory**","p":["Adjust batch size for training; for inference, modify settings in config.py. Consider upgrading the GPU if needed."]},{"i":"file-memory-error-when-training","l":"**File/memory error(when training)**","p":["Reduce \"Number of CPU threads\" or pre-cut training set to shorter audio files."]},{"i":"error-about-llvmlite-dll","l":"**Error about llvmlite.dll**","p":["Install vc_redist.x64.exe for Windows."]},{"i":"runtimeerror-the-expanded-size-of-the-tensor","l":"**RuntimeError: The expanded size of the tensor...**","p":["Delete smaller-sized wav files, then click \"train the model\" and \"train the index.\""]},{"i":"runtimeerror-the-size-of-tensor-a-must-match","l":"**RuntimeError: The size of tensor a must match...**","p":["Avoid changing the sampling rate mid-training. If necessary, change the exp name and train from scratch or copy pitch and features folders to accelerate training."]},{"i":"the-filename-directory-name-or-volume-label-syntax-is-incorrect","l":"**The filename, directory name, or volume label syntax is incorrect**","p":["Make sure to correctly set the path for your dataset before training. Remember that the dataset should not be in a zip file within the Applio/assets/datasets folder."]},{"i":"indexerror-list-index-out-of-range","l":"**IndexError: list index out of range**","p":["Make sure to use the same sample rate when retraining or when selecting a custom pretrain."]},{"i":"alternative","l":"**Alternative**","p":["If all else fails, download the precompiled (Zip or Exe) version. This will bring everything ready to start applio without the need to perform the installation."]}],[{"l":"Info","p":["Common questions and information about RVC/Applio in general"]},{"i":"how-to-train-and-infer-without-the-webui","l":"**How to train and infer without the WebUI?**","p":["Link: Training and Inference without WebUI"]},{"i":"how-to-share-a-model-how-to-use-others-models","l":"**How to share a model/How to use others models?**","p":["Share the pth file in the zips folder, not the one in logs. Do not copy large pth files; use the ckpt tab to extract a smaller model for sharing. Compress it into a zip and upload it to Drive or Hugging Face if you want to publish your model in Ai Hispano, go to the \uD83D\uDCE9 publish-a-model section and execute the /model command and fill in the corresponding data of your model"]},{"i":"test-previous-model-and-continue-training","l":"**Test previous model and continue Training**","p":["select one of the .pth generated file in Applio/Logs/zips(you need to have the index previously generated for best results) or Save via model extraction at the bottom of the ckpt processing tab.","Add data to a new path, process dataset, extract features, copy G and D files from the previous experiment, and continue training.","when increasing the number of audio, it is recommended to retrain the model so as not to have a mixture of datasets."]}],[{"l":"Others"},{"l":"Check Build Tools Installed","p":["How to Verify the Correct Installation of Visual Studio Build Tools","If you encounter issues with the installation of Visual Studio Build Tools, you can ensure they are installed correctly by following these steps:","Open the Visual Studio Installer. If you haven't installed it yet, you can download it here.","Confirm that the displayed screen resembles the one shown below (or a similar one for a different version):","Visual Studio Installer","2.1. If the required components are not installed, navigate to the \"Available\" tab. Click on the \"Install\" button next to \"Visual Studio Build Tools 2022\" and select both \"Desktop development with C++\" and \".NET Desktop Buildtools\" by clicking on their respective icons:","If the screen resembles the one above, click on the \"Modify\" button. Ensure that both \"Desktop development with C++\" and \".NET Desktop Buildtools\" options are selected. If they are not, click on them, and then click the \"Install\" button:"]}],[{"l":"RVC","p":["Questions about RVC."]},{"i":"what-is-rvc","l":"What is RVC?","p":["RVC (Retrieval-Based Voice Conversion) is an advanced AI voice cloning techniques using speech synthesis."]},{"i":"what-are-the-requirements-for-rvc-applio","l":"What are the requirements for RVC/Applio?","p":["The minimum required for local training is an Nvidia RTX series 20 graphics card with 8GB of VRAM to train models, in the case of inference you only need to have a decent CPU and at least 4GB of VRAM. If you have a Celeron processor, it would be better to look at other alternatives."]},{"i":"can-i-use-rvc-applio-on-a-mac","l":"Can I use RVC/Applio on a Mac?","p":["Yes, but only for inference. besides, the installation should be done as if it were on Linux."]},{"l":"Dataset","p":["A Dataset is a set of audio files compressed into a .zip file used by RVC for voice training."]},{"i":"is-a-dataset-the-same-as-a-model","l":"Is a Dataset the same as a Model?","p":["No, a Dataset is the set of audio used for training, while a Model is the result of that training."]},{"l":"Added index","p":["The added index contains the compressed dataset and is responsible for controlling the tones of the model at the time of inference."]},{"l":"Pth","p":["It is the one that contains all the data from the trained model that will serve us to make inference."]},{"l":"Epoch","p":["An Epoch is the number of iterations performed during training to complete one full cycle of your dataset. For example, if you have a dataset of 200 audio samples and you set a batch_size of 10, 10 audio samples will be processed in each iteration. To process all 200 audio samples, you will need to perform 20 iterations in total. This complete cycle is referred to as one epoch."]},{"l":"F0 extraction methods","p":["pm: Provides a fast result at the cost of vague tone processing.","Harvest: Better replication of tones, slower model and often gives errors.","Dio: Similar to PM processing, provides fast processing with better transition in tones.","Crepe/crepe-tiny: Contains an elaborate processing of pitch changes, compared to the previous ones this one does not distort in magnitude. (Crepe is to be able to modify the value of pitch processing (Hop Lenght), lower value will induce a better result but slower and vice versa.)","Rmvpe: Model with Crepe and PM features, fast and quality tone processing with low probability of distortion.","Fcpe: Fpce is a alternative hybrid method that works similarly to f0 with the difference of a slightly more optimized processing and taking less context from the input audio for a \"particular\" conversion."]},{"l":"Batch Size","p":["The batch size is the amount of GPU that will be used to train the model. The larger the batch size, the shorter the training duration. It is recommended to use multiples of 4 as the batch size, although the most common is 8, as it gives the AI time to learn correctly without rushing."]},{"l":"Inference","p":["is the process where an audio is transformed by the voice model."]},{"l":"Artifacting","p":["Is when the inference output sounds robotic, distorted, with background noise and with fails when trying to modulate words."]},{"l":"Pretrained","p":["It is a model trained with several sets of long-duration audios that will serve as a basis for training the models in RVC."]},{"l":"Overtraining","p":["It’s when the TensorBoard graph starts to rise and never comes back down. An overtrained model will sound robotic, muffled, and won’t be able to articulate words well. For more information check the Tensorboard section."]},{"l":"G and D","p":["They are responsible for storing the training data of the model. Generative learns to replicate results similar to the original. Discriminator tries to distinguish real data from those created by the generator."]}],[{"l":"LICENSE","p":["=======================================================================","Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.","Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.","Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.","Attribution-NonCommercial 4.0 International","Attribution.","By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.","Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.","Creative Commons Attribution-NonCommercial 4.0 International Public License","Creative Commons Corporation (\"Creative Commons\") is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \"as-is\" basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.","Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark \"Creative Commons\" or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.","Creative Commons may be contacted at creativecommons.org.","Creative Commons may be contacted at creativecommons.org. Attribution-NonCommercial 4.0 International","Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.","Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.","Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.","for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;","For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.","For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.","For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.","For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.","if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and","License grant.","Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.","Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.","Licensor means the individual(s) or entity(ies) granting rights under this Public License.","No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.","NonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.","Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.","Other rights.","Section 1 -- Definitions.","Section 2 -- Scope.","Section 3 -- License Conditions.","Section 4 -- Sui Generis Database Rights.","Section 5 -- Disclaimer of Warranties and Limitation of Liability.","Section 6 -- Term and Termination.","Section 7 -- Other Terms and Conditions.","Section 8 -- Interpretation.","Sections 1, 5, 6, 7, and 8 survive termination of this Public License.","Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.","Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.","The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.","The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.","This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.","To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.","TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.","UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.","Using Creative Commons Public Licenses","Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:","Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:","You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.","You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.","Your exercise of the Licensed Rights is expressly made subject to the following conditions."]}]]